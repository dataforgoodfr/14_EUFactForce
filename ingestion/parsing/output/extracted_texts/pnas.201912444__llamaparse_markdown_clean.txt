
# Misinformation in and about science

Jevin D. Westa,1 and Carl T. Bergstromᵇ

aInformation School, University of Washington, Seattle, WA 98195; and bDepartment of Biology, University of Washington, Seattle, WA 98195

Edited by Dietram A. Scheufele, University of Wisconsin–Madison, Madison, WI, and accepted by Editorial Board Member Susan T. Fiske October 11, 2020 (received for review November 15, 2019)

Humans learn about the world by collectively acquiring information, filtering it, and sharing what we know. Misinformation undermines this process. The repercussions are extensive. Without reliable and accurate sources of information, we cannot hope to halt climate change, make reasoned democratic decisions, or control a global pandemic. Most analyses of misinformation focus on popular and social media, but the scientific enterprise faces a parallel set of problems—from hype and hyperbole to publication bias and citation misdirection, predatory publishing, and filter bubbles. In this perspective, we highlight these parallels and discuss future research directions and interventions.

misinformation | disinformation | fake news | data reasoning | science communication

Misinformation has reached crisis proportions. It poses a risk to international peace (1), interferes with democratic decision making (2), endangers the well-being of the planet (3), and threatens public health (4, 5). Public support for policies to control the spread of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is being undercut by misinformation, leading to the World Health Organization’s “infodemic” declaration (6). Ultimately, misinformation undermines collective sense making and collective action. We cannot solve problems of public health, social inequity, or climate change without also addressing the growing problem of misinformation.

Most of the research efforts and interventions examine broad, public consumption of misinformation—modeling the spreading dynamics of falsehoods (7, 8), examining social network effects (9, 10), and evaluating crowd-sourced mediation (11), with a special focus on crisis events (12) and political elections (13). In this article, we turn the spotlight on science. We look at the ways that misinformation can travel within science due to misaligned incentives, out-of-date publishing norms, and sociotechnical systems that concentrate attention and credit on a small subset of the literature.

Appealing as it may be to view science as occupying a privileged epistemic position, scientific communication has fallen victim to the ill effects of an attention economy. This is not to say that science is broken. Far from it. Science is the greatest of human inventions for understanding our world, and it functions remarkably well despite these challenges. Still, scientists compete for eyeballs just as journalists do. They face incentives to hype their work and to publish selectively those findings that are surprising and “clickable.” Like other information consumers and producers, researchers rely on search engines, recommendation systems, and social media to find relevant information. In turn, scientists can be susceptible to filter bubbles, predatory publishers, and undue deference to the authority of numbers, P values, and black box algorithms.

# Hype and Hyperbole

The internet has changed the way we interact with the media. Some of us still read the morning paper at the breakfast table, but many more get the majority of their information on the internet. More people consume news online through social media (20%) or online news sites (33%) than in print form (16%) (14), especially among younger readers (15). While nearly half of Americans still get the bulk of their news from mum wage rates and unemployment trends possibly compete with yet another celebrity breakup, the one hygiene trick your dentist does not want you to know, or nine cats who look like characters from The Office? Publishers need flair, fluff, and sparkle to draw our attention—and they have responded enthusiastically.

Nuance falls by the wayside. Headlines often replace factual statements with promises of emotional experiences. Highly shared Facebook headlines pledge to “make you cry tears of joy,” “give you goosebumps,” or “melt your heart” (19). Rather than summarize the contents of the story, headlines deliberately obscure them to incite a click: “How to avoid the leading cause of death”; “Do economists think the Fed will cut rates?” Forward reference headlines (20) exploit our curiosity (21) by replacing key pieces of information with forward-referring pronouns. The reader has to click the story to discover their referents: “These pets are adorable but may carry a deadly disease”; “One-fifth of this occupation has a serious drinking problem”; “Many scientists overlook this crucial detail when reading PNAS.”

Parallel changes have taken place in the way that scientists write and read scholarly articles. Two decades ago, we received hard copies of journals and browsed through them regularly. Now, we mostly find articles using search engines or sometimes through social media. The result is a head-to-head competition between journal articles that parallels the competition among news stories on a smartphone. Among scientific papers, titles

This paper results from the Arthur M. Sackler Colloquium of the National Academy of Sciences, “Advancing the Science and Practice of Science Communication: Misinformation About Science in the Public Sphere,” held April 3–4, 2019, at the Arnold and Mabel Beckman Center of the National Academies of Sciences and Engineering in Irvine, CA. NAS colloquia began in 1991 and have been published in PNAS since 1995. From February 2001 through May 2019, colloquia were supported by a generous gift from The Dame Jillian and Dr. Arthur M. Sackler Foundation for the Arts, Sciences, & Humanities, in memory of Dame Sackler’s husband, Arthur M. Sackler. The complete program and video recordings of most presentations are available on the NAS website at http://www.nasonline.org/misinformation about science.y

Author contributions: J.D.W. and C.T.B. performed research and wrote the paper.y

The authors declare no competing interest.y

This article is a PNAS Direct Submission. D.A.S. is a guest editor invited by the Editorial Board.y

Published under the PNAS license.y

1To whom correspondence may be addressed. Email: jevinw@uw.edu.y

Published April 9, 2021.

# SOCIAL SCIENCES COLLOQUIUM PAPER





with positive, more interesting framing receive higher Altmetric scores (22).

Scientists likely feel increased pressure to hype their results because productivity metrics have taken on a greater role in scientific advancement (23). A publication is no longer merely a way of reporting results; it is a coveted prize that can make or break an early career (24). In some countries, a publication in a top venue draws bonuses—in China, up to $165,000 US dollars (25), although this practice was recently banned (26). Given that top journals often look for exciting results of broad impact, these policies encourage researchers to hype their work. Worse still, they may encourage fraud.

During a crisis, science can be forced into the media spotlight. Eager to accelerate the research cycle during the ongoing pandemic, scientists are making extensive use of preprint servers for polished papers and preliminary work alike (27). This can be a valuable mode of communication among researchers, but because it takes place in the open, journalists pick up on the work and do not always approach the findings with sufficient caution.

For example, there is no credible evidence that SARS-CoV-2 responsible for the COVID-19 pandemic has a bioengineered origin, but a series of preprints has pushed false narratives along these lines. One such paper, posted to bioRxiv (28), was quickly refuted by bioinformaticians and formally withdrawn—but in the interim, the paper received extensive media attention. If preprint servers try to vet the material, authors find other outlets. A two-page note—not even a research paper—claimed that SARS-CoV-2 is an escaped bioweapon and was posted to the academic social media platform ResearchGate (29). Though quickly deleted from the site, this document took off, particularly within conspiracy circles. A deeply flawed paper making similar arguments was posted to the file-sharing site https://zenodo.org/ (30). It received considerable attention after the author appeared on cable news promoting the claims and the US president tweeted a video clip of a cable news host praising the work (31).

Increasingly, we see research being released to the media prior to any publication even available for critique. Controversial work on detecting sexual orientation from photographs using artificial intelligence (32) was reported in The Economist and The Guardian before a preprint or white paper was available. Reports went viral about a research paper on the spread of COVID-19 by respiratory droplets from joggers—but no such paper existed, only an animated computer visualization (33). Controversial results from a Los Angeles County COVID-19 seroprevalence study were reported worldwide based on a press conference (34), but detailed information about methods and results was unavailable until weeks later.

In addition, researchers commonly misstate or overstate the implications of their work (35). In concert with researchers, university press offices play a particularly important role in communicating science—but too frequently do so in ways that prioritize web traffic over accuracy. Sometimes spin is carried over from the journal article itself (36); other times, it is added in the press release. A biomedical report might omit important caveats, draw inappropriate extrapolations from mouse models, and exaggerate prescriptive implications. One analysis found that nearly a third of 525 papers in top obesity or nutrition journals make inappropriate causal claims in their abstracts or titles (37); in another study, roughly the same fraction of health-related papers widely shared on social media used inappropriately strong causal language (38). Some fields may be more prone to hype than others. A new result on the geometry of Banach spaces may be more difficult to hype than a bioweapon claim, but we surmise that most fields are susceptible.

Much of this truth bending may be unnecessary. Most studies (39–41) fail to find an association between exaggeration and uptake by the news media. Admittedly, selection bias may play a role: perhaps the stories that are not exaggerated are those that do not need to be. In any case, high-quality press releases appear to drive higher-quality news stories on that research (42, 43).

# Publication Bias

If you get your ideas about risk and safety from watching crime dramas—or even the local news—you probably think the world is a dangerous place (44). Intruders attack sleeping homeowners, children are kidnapped, and museums are burgled. “If it bleeds, it leads”—news outlets eager to attract views know that frightening stories of danger and tragedy capture our attention. We all want to learn what circumstances to avoid. The stories do not even have to be true; we all tend to talk about what scares us. In the 1970s and 1980s, urban legends about razor blades in apples led local police stations to set up X-ray machines for scanning Halloween treats and drove some communities to contemplate trick-or-treating bans—despite the fact that the scare was almost entirely fictitious (45, 46). An urban legend from our youth, about a parking lot slasher who hides under cars to slice his victims’ Achilles tendons (47), has reemerged in 2020, updated for the social media platform TikTok with a new twist about human trafficking (48).

Science features an analogous filtering process, though the bias trends toward good news rather than bad. One of the more disturbing realizations of the past decade is that many established scientific results in the social (49, 50) and biomedical (51–53) sciences cannot readily be replicated. This so-called “replication crisis” has been driven in part by the incentive structure of scientific publishing. Journals preferentially publish positive results with statistically significant outcomes. Scientists who obtain negative results or nonsignificance may choose to move on to another project rather than to invest in writing and publishing work thought to be of only modest interest. The result is publication bias, whereby the published literature provides a biased sample of the research actually conducted (54). With negative results buried in file drawers (55), conclusions drawn from the published record can be misleading, and “false facts” can become canonized in the literature (56).

How bad is the problem? We do not really know. It is relatively straightforward to measure the fraction of published results that are negative. One study found that only 15% of results published across the sciences are negative, with even lower levels in some fields such as ecology and psychology (57). However, to evaluate the effect of publication bias, we need to know what fraction of negative results is unpublished. To get at this more difficult estimate, Turner et al. (58) compared the Food and Drug Administration (FDA) registrations of antidepressant clinical trials with the published record in biomedical journals. In the published literature, 94% of the reported trials obtained positive results of drug efficacy. However, looking at the original registrations and the results as reported to the FDA, the team saw a different picture. Only 51% of the studies yielded definitively positive results according to the original outcome measures. Why the discrepancy? Almost all of the positive results were published, whereas fewer than half of the questionable or negative results were published. Moreover, many of the questionable or negative results were recast as positive via “outcome switching,” the questionable practice of reporting different outcome measures than those specified in the original trial registration. Reading the published literature, you would think antidepressants were ubiquitously effective. Seeing the full picture, the prognosis is more nuanced.

In response, researchers and publishers are beginning to experiment with registered reports (59, 60). Under this publishing model, reviewers evaluate proposed studies before they are conducted and offer in-principle acceptance: irrespective of the results, the study will be published if properly conducted.





# Advocates suggest that reviewing proposals instead of completed experiments will create a more reliable literature

Both by reducing the incentive for scientists to mine data for surprising findings and by reducing publication bias against negative results. However, we do not see preregistration as a panacea. It may not be appropriate for all types of research; it discourages exploratory research, which can generate important, unexpected findings, and there is little evidence to date that it will appreciably reduce publication bias (61–63).

A form of publication bias arises in popular science reporting as well. News media eagerly report potential breakthroughs, often failing to clearly indicate their preliminary nature. COVID-19 reporting is no exception (33, 64). The withdrawn bioRxiv preprint mentioned previously was promoted so broadly that it garnered one of the highest Altmetric scores of all time (28). As another example, a Financial Times headline proclaimed “Coronavirus may have infected half of UK population—Oxford study,” even though it was reporting on a preliminary white paper that neither showed nor attempted to show anything of the sort (65, 66).

Intentions are difficult to measure; it is likely that many of these citations are due to honest mistake or laziness rather than deliberate obfuscation. When a paper misrepresents the papers it cites, this can be grounds for retraction (77). A bigger problem arises when one paper is frequently misrepresented by no fault of its own. In one notable case, a short letter published in the New England Journal of Medicine reported on opioid use and addiction among patients at the Boston University Medical Center (78). The authors concluded that “despite widespread use of narcotic drugs in hospitals, the development of addiction is rare in medical patients with no history of addiction.” This five-sentence letter has been cited over 600 times, most often as evidence for the incorrect assertion that opioids are not addictive. As of 2017, fewer than 20% of those citations acknowledged that the report was restricted to the hospital setting and does not apply to the in-home use where much of opioid addiction arises (79).

Retracted papers are frequently cited as legitimate even after retraction. In a recent study in radiation oncology, Daniel Hamilton (80) found that 92% of articles citing retracted articles subsequent to retraction cited them as if the retraction had never occurred. Presumably, this stems primarily from a lack of awareness, not deceitful intentions. The website retractionwatch.org/ lists the mostly highly cited retracted articles. A few observations are that retracted papers come from top-tier journals including New England Journal of Medicine, Science, and The Lancet; the top papers are cited thousands of times; and some papers are actually cited more after retraction than before retraction.

Citation bias is a related phenomenon, in which the claims associated with citations accurately report the results—but authors preferentially cite papers that support a claim over those that undermine it (81–84). Citation bias exacerbates the problems created by publication bias. If authors preferentially write up positive results and journals preferentially publish them, the citation record will be biased toward positive results even for incorrect hypotheses. If researchers also more likely cite positive results, the citation record will further distort our view of experimental findings.

# Fake News and Predatory Publishing

The most successful fake story of 2016, “Pope Francis Shocks World, Endorses Donald Trump for President,” was published and spread by Macedonian teenagers who did not care a whit whether Trump or Clinton won the election (85). They were simply trying to generate advertising revenue—and they were wildly successful, bringing in hundreds of thousands of dollars. This type of exploit became possible because of massive shifts in communication technology and associated economic structures for monetizing information. When the revenue model for news was based on subscriptions and circulation, there was little value to publishing a single catchy article; one needed an established paper, magazine, radio station, or television channel.





to the internet, authenticity was also hard to spoof. What mali- cious agent could print a million copies of a fake newspaper or take over television bandwidth with professional-quality broad- cast content? Finally, how to attract readers or viewers? The onus was on the publisher to grow an audience through adver- tising and other costly measures. Social media and online ad revenue models allow anonymous or previously unknown actors to create and make money from content that can reach tens of millions of people.

A similar racket operates within the scientific ecosystem, in the guise of predatory publishers. Again, a shift in information tech- nology made this possible. Digital typesetting and online distribu- tion make authenticity easy to spoof: with a bit of know-how and a few days’ work, one can put together a website that looks like that of a scientific publisher. Changing economic models created new opportunities for malfeasance. The rise of electronic distribution established a market for online open access, in which the costs of publishing are borne by the authors instead of the readers. While the open access model has numerous advantages (86), it also results in a transfer of purchasing decisions from highly trained, highly motivated librarians deciding on journal subscriptions to untrained and heterogeneously motivated authors shopping for venues in which to publish single articles (87).

Predatory publishers are not invested in the gate-keeping, curation, and manuscript improvement roles of traditional jour- nal publishers. They are focused on collecting open access pub- lication fees, the funds that authors pay to make their work available to the world without subscription charges. How serious is the problem? According to one study (88), predatory publish- ers produced nearly half a million articles in 2014, bringing in around $74 million in publication fees. For comparison, the esti- mated market for reputable open access journals is around $250 million annually, and the number of articles in the Web of Sci- ence in 2014 was about 2.5 million. When including the entire literature, predatory publishing likely comprises about 5 to 10%.

So why do authors publish in these venues? Some authors may be duped by spam emails, but we suspect that in many cases, researchers are complicit. Scientists face strong pressures to pub- lish frequently. With minimal or nonexistent peer review, preda- tory publishers offer an easy route to rapid publication (89). Thus, a predatory publisher may not need to fool prospective authors about its legitimacy. The publisher instead may be offer- ing authors an opportunity to fool any bureaucracy or committee that assesses productivity by merely counting publications.

Yet more worrisome are the ways in which these publications mislead the public. Con artists publish fabricated or otherwise deceptive trials of snake oil therapies and use the publica- tions in their sales pitches. The unapproved cancer treatment, Gc protein-derived macrophage activating factor (GcMAF), has been touted in several predatory journals (90). Denialists of var- ious stripes—antivaxxers, creationists, HIV denialists, climate skeptics, chemtrail believers—use these venues for “peer review” legitimacy. This can be confusing to a public that has little training in detecting imposter science.

Scientists and the public need better ways of detecting untrust- worthy publishers. We have developed methods for identifying suspicious journals that are exceedingly costly given their low influence (87), but more needs to be done to spot fictitious edi- torial boards and recently assigned web domains. Ultimately, the best solution will be to train the next generation of scien- tists, journalists, and the public to recognize legitimate scientific research (for a primer, see https://callingbullshit.org/tools/toolslegit.html).

# Filter Bubbles and Echo Chambers

In the midtwentieth century, we relied on Edward Murrow and Walter Cronkite for nightly news. The rise of cable tele- vision and the 1987 repeal of the Federal Communication Commission’s fairness doctrine set into motion an increasing polarization of news (91). Today, algorithms learn to select content that our friends share, feeding us what we want to hear and not always what we need to know. As a result, we may be retreating into proverbial “filter bubbles” or “echo chambers,” despite increased access to diverse ideas, sources, and opinions. Some studies observe reinforcement of this sort (92); others provide conflicting evidence in both magnitude and direction (93–96).

Just as in society, gatekeepers are changing in science. Tra- ditionally, journals have been the primary arbiters of content. Editors pick candidate papers; reviewers adjudicate. That has been the basic model for the last half century (97). However, over the past two decades, a new information milieu has emerged. Preprint archives, academic search engines, article recommen- dation systems, and social media do not require bound journals to deliver content. In this new communication environment, do journals still matter as gatekeepers, and do echo chambers exist in science?

In a recent study, we tracked citations of papers published on the arXiv before and after journal publication (98). After con- trolling for article quality, we find that arXiv articles published in higher-ranked journals received more citations than articles pub- lished in lower-tier journals. This indicates that journals retain gatekeeper roles for consumers. For producers, the story changes somewhat. We find that papers highly cited as preprints are less likely to be published in journals at all (98).

Changes in the curation and delivery of scholarly content extend beyond journals. Are search engines and recommender systems promoting epistemic diversity, or are they narrowing our view of the literature? One could easily imagine it going either way. Online access lowers the search cost of obtaining most articles; search engines and recommendation systems reduce the reliance on disciplinary journals. Thus, we might not be surprised that some studies have found that scientists read more broadly than previously (99). However, search engines such as Google Scholar return articles in an order influenced by previous citation counts and related criteria. This could eas- ily accentuate a form of the Matthew Effect (100, 101) in which frequently cited papers attract an increasingly disproportionate share of citations as their fame grows. In our own investigations (102), we find minimal changes when correcting for marginals bias, which counters previous findings that show a narrowing of citation distributions (103, 104), but this result varies across disciplines.

Viewpoint diversity is important for science (105–107), so bet- ter understanding technology’s impact on this diversity is needed. In particular, we need to better understand the systemic effects of search engines on the literature. Google Scholar is one of the most important tools in science (108). Yet, the tool is a black box; the rules for ordering results are a mystery; the algorithms are continually changing, obviating any hope of reproducibility; the corpus is unknown, and estimates of its size differ dramati- cally (108); it is nonextensible and minimally customizable; and there has been little effort by Google Scholar to engage with researchers. Fortunately, there has been a flurry of development from other academic search engines including Semantic Scholar, Microsoft Academic Graph, Web of Science, and others.

# Data and Science Distortion

Our world is quantified to an unprecedented degree. Our cell phones track our every move; arrays of ambient sensors monitor our cities; the internet of things tallies our domestic activity; and data exhaust from our online lives provides intricate detail about our interests, needs, and desires. Readily available data play an increasingly important role in decision making and public com- munication alike—but often, those data are misinterpreted by accident or cherry-picked to promote specific agendas.





Yet for all of the importance of data in contemporary decision making, we tend to associate misinformation with fake news or snake oil and less often think about how data—even accurate data—can misinform. Data appear objective, precise, and replicable but offer a near-endless array of presentations, framings, and comparisons that can be used to tell a wide range of stories. Matters get even worse with data visualization: choice of type, the scales and ranges of the axes, the bin sizes of histograms, the presence or absence of visual decoration, and other graphical conceits can influence a story in any direction a designer may desire (109–111). Without training, readers can be fooled readily. One recent study found that poor numerical literacy was associated with higher susceptibility to COVID-19 misinformation (112).

One of the most direct ways that numbers mislead is unfair comparison. For example, in the popular An Inconvenient Truth documentary about climate change, Al Gore showed increased monetary damages due to hurricanes (113). The data were correct, but costs were not corrected for inflation and rising home prices in coastal areas. Making these adjustments, the massive increase in hurricane damage largely disappears.

Even with the best of intentions, researchers can stumble when interpreting their data. Researchers try to navigate around statistical traps, including selection bias and confounds (114), data censoring (115), Simpson’s paradox (116), Will Rogers effect (117), and observation selection effects (118). The ubiquitous but oft-misused P value even received a formal statement of caution from the American Statistician (119). With so many potential pitfalls, every statistical analysis deserves careful scrutiny. We need to better understand the scope across which numeric research findings can be generalized. While we often have intuitions about this, new work is finding ways to formalize it (120).

In the meantime, purveyors of propaganda go out of their way to create doubt even where it is unmerited. The field of agnotology studies how business interests, governments, and other agencies systemically create doubt around scientific findings and manipulate what we know and do not know about science (121). Whether designed to discredit the link between tobacco and cancer or to deny the reality of anthropogenic climate change, efforts at agnotogenesis—creating and spreading doubt—use a similar playbook (122). The aim is rarely to disprove the undesirable facts but rather, to induce sufficient doubt to “keep the controversy alive” and thereby, stave off regulatory action. The smoking gun is there for everyone to see; the goal is to provide people with alternative reasons to believe it might be smoking.

The “falsehood firehose” is another strategy that pushes huge volumes of self-contradictory disinformation (123), meant to deceive, confuse, disorient, and agitate (124, 125). The goal is not to promote one particular untruth but instead, to so thoroughly confound truth and falsehood that confidence in institutions—and even in the notion of truth itself—is undermined (123, 126). Recently, we have seen this approach adopted by science denialist factions as well (127). While perhaps accidental, the bungled COVID-19 risk communications out of the White House during February had similar effects. In late February 2020, for example, the president and director of the National Economic Council assured the US public that the epidemic had already been contained—at the same time as the director of the CDC was trying to brace the US public for extensive domestic spread and substantial disruption to everyday life. These and related blunders contributed to a growing sense of bewilderment and distrust toward the public health community.

# Interventions

So what can we do about misinformation in and about science? Volumes have been written on regulatory, technological, and educational approaches to online misinformation (128–130), but this literature has largely focused on society broadly construed rather than on science in particular.

As a start, we should focus on incentives. The so-called New Economics of Science (131–133) models scientists as approximately rational actors motivated by nonepistemic considerations such as prestige and salary. Using this approach, we might be able to improve the efficiency of the scientific process by nudging science’s norms and institutions in the right directions. The aim is to create incentives that are compatible with the behaviors we want to encourage and that discourage the behaviors we want to eliminate (134).

Much of the present pathology of hype, hyperbole, and publication bias is associated with an overreliance on productivity metrics (23). Researchers, journals, and institutions are subjected to high-stakes quantification, from hiring to promotion and funding (135, 136). Goodhart’s law predicts the consequences. Restated concisely by Marilyn Strathern (137), the law observes that “when a measure becomes a target, it ceases to be a good measure.” Because universities and scientists are measured on these metrics, they face strong pressure to publish at high rates, and journal prestige takes on an inordinate significance (138). Scientific papers are “salami-sliced” into minimal publishable units, and claims are oversold. Though full-on P hacking may not be all that common (139), questionable research practices abound (140, 141), and the scientific enterprise rewards them (142–144) (but see ref. 145).

The peer review system is overtaxed by the volume of papers being written, and in many fields, there is no way for researchers to read the literature exhaustively (146). Changing the incentives around publication would help. Hiring committees, promotion committees, and funding agencies would do well to look closely at some fixed number of publications, thereby creating incentives for researchers to publish a smaller number of higher-quality papers (147).

We need to develop methods for identifying errors and statistical anomalies (148). We need to consider integrating preregistration (where appropriate) as standard practice to reduce the effects of publication bias, continue to develop tools for open science, and reward those scientists that adhere to these new standards. We need to encourage researchers to broaden their search platforms to reduce a possible “Google Scholar bubble.” We need better ways to evaluate reference lists to reduce citation errors. References are used not only by researchers but also, as primary input for search engine algorithms (149). They affect both the consumption and production of the literature. This may require an independent step in an already overtaxed peer review system, whereby additional reviewers examine only the citations. We need to do a better job helping the public identify legitimate science venues and strongly discourage scientists from publishing their research in predatory journals. Additionally, we need more science writers both within and outside science institutions. In 2009, there were only 79 full-time science reporters at newspapers in the United States (150). This paucity of science writers likely impacts public perception of, understanding of, and interest in science.

As society increasingly relies upon quantitative data, data reasoning skills become paramount. In 2017, we began developing a curriculum to address these issues of quantitative literacy (151). Our aims are twofold. First, we seek to teach students from nonscience, nonquantitative backgrounds how to hold their own in a data-driven society. We aim to dissolve the myth of numbers as impartial, hard, and unbiased; we show our students how to question numbers without technical training; and we do this, importantly, with a focus on how science works. Second, we aim to redress a major oversight in science, technology, engineering, and mathematics (STEM) education. In our experience, students develop impressive technical proficiency in coding, calculating, and conducting laboratory procedures. They less often receive adequate training in the elements of critical and humanistic thinking that underlie the productive use of these skills.

# SOCIAL SCIENCES COLLOQUIUM

# PAPER





Our class fills with students from more than 40 different majors, including many in the arts and humanities. We have shared our teaching materials with faculty from across the disciplinary landscape working at dozens of universities across the globe, and a number of universities now offer a similar course.

In our class, we present students with a simple schema for reasoning about data. Whether we are looking at statistical methodology, machine learning algorithms, or any other modes of data processing, there is a common structure to the analysis. First, data are collected. These go into a “black box” wherein the technical operations occur: logistic regression, random forest algorithm, or some other technology. The block box spits out summary statistics, data classifiers, or other forms of output. From that output, the investigator then derives various conclusions and interpretations. The black box may be inscrutable to most readers, but that is all right. Often, one does not need to open the box—to delve into the formal mechanics—to think critically about the analysis. When something goes wrong, the problem seldom resides within the black box (i.e., it is seldom a technical artifact of the analysis). Far more often, the data are flawed or unrepresentative, or the conclusions and interpretation are unjustified. Students do not need a great deal of technical training to spot these problems. Instead, we stress concepts such as selection bias, correlation vs. causation, relative vs. absolute risk, and plausibility checking via Fermi estimation.

For all these interventions, few will be effective if the public distrusts science. Pew Foundation surveys* of US residents have revealed declining trust in government, religious organizations, universities, business leaders, news media, and fellow citizens, with young people exhibiting particularly low levels of trust (152, 153). Fortunately, science remains among the few trusted institutions in the United States (154–157); however, that trust is declining in some regions and among some political orientations (158, 159).

Public engagement and understanding of science should be a priority for all scientists. This is not a matter of just teaching more astronomy or biology. Rather, it involves nurturing innate curiosity and teaching people to understand how science works, how to consider evidence when making conclusions, and how popular media distorts these conclusions. In our class, we spend nearly a quarter of our time talking about the nature of science and about the issues we have described here, from publication bias to predatory journals. We stress that while science has its problems, it incorporates mechanisms to correct mistakes. In our efforts, we have been inspired by the many other related courses developed elsewhere, notably Sense and Sensibility and Science at the University of California, Berkeley and Think Critically at the University of Texas at Austin and the University of Idaho.

We are optimistic that science and society alike will survive their immersion into new information technologies—but this will require education efforts in media literacy, data reasoning, and the philosophy of science. It will require policy makers and funders to support both research and public outreach, especially in rural regions of the world and in marginalized populations. Most importantly, this all needs to be done with a recognition that science relies on public trust for its funding and opportunities to interface with the world. Misinformation in and about science could easily undermine this trust. We cannot afford to let that happen.

# Data Availability.

There are no new data associated with this article.

# ACKNOWLEDGMENTS.

The research highlighted was supported by NSF Award 1735194 and the Knight Foundation. We thank three anonymous reviewers and the editor for useful comments and questions. We published a trade book on misinformation with Penguin Random House in 2020.

| 1.  | R. Goldman, Reading fake news, Pakistani minister directs nuclear threat at Israel. NY Times, 24 December 2016. <https://www.nytimes.com/2016/12/24/world/asia/pakistan-israel-khawaja-asif-fake-news-nuclear.html>. Accessed 7 December 2020.                                                                                                               |
| --- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 2.  | US Senate, “Select committee intelligence United States Senate: Russian active measures campaigns and interference in the 2016 U.S. election” (Rep. 116–290, US Government, 2019).                                                                                                                                                                           |
| 3.  | S. Van der Linden, A. Leiserowitz, S. Rosenthal, E. Maibach, Inoculating the public against misinformation about climate change. Global Challenges 1, 1600008 (2017).                                                                                                                                                                                        |
| 4.  | D. A. Salmon, M. Z. Dudley, J. M. Glanz, S. B. Omer, Vaccine hesitancy: Causes, consequences, and a call to action. Vaccine 33, D66–D71 (2015).                                                                                                                                                                                                              |
| 5.  | D. L. Chi, Parent refusal of topical fluoride for their children: Clinical strategies and future research priorities to improve evidence-based pediatric dental practice. Dent. Clin. 61, 607–617 (2017).                                                                                                                                                    |
| 6.  | J. Zarocostas, How to fight an infodemic. Lancet 395, 676 (2020).                                                                                                                                                                                                                                                                                            |
| 7.  | M. Del Vicario et al., The spreading of misinformation online. Proc. Natl. Acad. Sci. U.S.A. 113, 554–559 (2016).                                                                                                                                                                                                                                            |
| 8.  | S. Vosoughi, D. Roy, S. Aral, The spread of true and false news online. Science 359, 1146–1151 (2018).                                                                                                                                                                                                                                                       |
| 9.  | C. T. Bergstrom, J. B. Bak-Coleman, Gerrymandering in social networks. Nature 573, 40–41 (2019).                                                                                                                                                                                                                                                             |
| 10. | A. J. Stewart et al., Information gerrymandering and undemocratic decisions. Nature 573, 117–121 (2019).                                                                                                                                                                                                                                                     |
| 11. | G. Pennycook, D. G. Rand, Fighting misinformation on social media using crowdsourced judgments of news source quality. Proc. Natl. Acad. Sci. U.S.A. 116, 2521–2526 (2019).                                                                                                                                                                                  |
| 12. | K. Starbird, J. Maddock, M. Orand, P. Achterman, R. M. Mason, “Rumors, false flags, and digital vigilantes: Misinformation on Twitter after the 2013 Boston marathon bombing” in IConference 2014 Proceedings (iSchools, 2014).                                                                                                                              |
| 13. | H. Allcott, M. Gentzkow, Social media and fake news in the 2016 election. J. Econ. Perspect. 31, 211–36 (2017).                                                                                                                                                                                                                                              |
| 14. | E. Shearer, Social media outpaces print newspapers in the U.S. as a news source. Pew Research Center (10 December 2018). <https://www.pewresearch.org/fact-tank/2018/12/10/social-media-outpaces-print-newspapers-in-the-u-s-as-a-news-source/>. Accessed 7 December 2020.                                                                                   |
| 15. | J. M. Twenge, G. N. Martin, B. H. Spitzberg, Trends in us adolescents’ media use, 1976–2016: The rise of digital media, the decline of TV, and the (near) demise of print. Psychol. Pop. Media Cult. 8, 329–345 (2019).                                                                                                                                      |
| 16. | A. M. Guess, B. Nyhan, J. Reifler, Exposure to untrustworthy websites in the 2016 US election. Nat. Hum. Behav. 4, 472–480 (2020).                                                                                                                                                                                                                           |
| 17. | A. Guess, J. Nagler, J. Tucker, Less than you think: Prevalence and predictors of fake news dissemination on Facebook. Sci. Adv. 5, eaau4586 (2019).                                                                                                                                                                                                         |
| 18. | B. Gardiner, You’ll be outraged at how easy it was to get you to click on this headline. Wired, 18 December 2015. <https://www.wired.com/2015/12/psychology-of-clickbait/>. Accessed 7 December 2020.                                                                                                                                                        |
| 19. | S. Rayson, BuzzSumo research: 100 Mil headlines analysis. Here’s what we learned. Buzzsumo (26 June 2017). <https://buzzsumo.com/blog/most-shared-headlines-study/>. Accessed 7 December 2020.                                                                                                                                                               |
| 20. | J. N. Blom, K. R. Hansen, Click bait: Forward-reference as lure in online news headlines. J. Pragmat. 76, 87–100 (2015).                                                                                                                                                                                                                                     |
| 21. | G. Loewenstein, The psychology of curiosity: A review and reinterpretation. Psychol. Bull. 116, 75–98 (1994).                                                                                                                                                                                                                                                |
| 22. | G. Lockwood, Academic clickbait: Articles with positively-framed titles, interesting phrasing, and no wordplay get more attention online. Winnower (29 June 2016). <https://thewinnower.com/papers/4892-academic-clickbait-articles-with-positively-framed-titles-interesting-phrasing-and-no-wordplay-get-more-attention-online>. Accessed 7 December 2020. |
| 23. | M. Fire, C. Guestrin, Over-optimization of academic publishing metrics: Observing Goodhart’s law in action. GigaScience 8, giz053 (2019).                                                                                                                                                                                                                    |
| 24. | A. E. Attema, W. B. Brouwer, J. Van Exel, Your right arm for a publication in AER? Econ. Inq. 52, 495–502 (2014).                                                                                                                                                                                                                                            |
| 25. | A. Abritis, A. McCook, R. Watch, Cash bonuses for peer-reviewed papers go global. Science 357, 541 (2017).                                                                                                                                                                                                                                                   |
| 26. | S. Mallapaty, China bans cash rewards for publishing papers. Nature 579, 18 (2020).                                                                                                                                                                                                                                                                          |
| 27. | P. Soltani, R. Patini, Retracted COVID-19 articles: A side-effect of the hot race to publication. Scientometrics 125, 819–822 (2020).                                                                                                                                                                                                                        |
| 28. | P. Pradhan et al., Uncanny similarity of unique inserts in the 2019-nCoV spike protein to HIV-1 gp120 and Gag. <https://doi.org/10.1101/2020.01.30.927871> (2 February 2020).                                                                                                                                                                                |
| 29. | B. Xiao, L. Xiao, The possible origins of 2019-nCoV coronavirus. <https://chanworld.org/wp-content/uploads/wpforo/default-attachments/1581810860-447056518-Originsof2019-NCoV-XiaoB-Res.pdf>. (6 February 2020).                                                                                                                                             |
| 30. | L.-M. Yan, W. Kang, J. Guan, S. Hu, Unusual features of the SARS-CoV-2 genome suggesting sophisticated laboratory modification rather than natural evolution and delineation of its probable synthetic route. (14 September 2020).                                                                                                                           |
| 31. | A. Ward, The bogus Steve Bannon-backed study claiming China created the coronavirus, explained. Vox (18 September 2020). <https://www.msn.com/en-us/news/world/the-bogus-steve-bannon-backed-study-claiming-china-created-the-coronavirus-explained/ar-BB19buS6>. Accessed 7 December 2020.                                                                  |





# SOCIAL SCIENCES COLLOQUIUM PAPER

1. Y. Wang, M. Kosinski, Deep neural networks are more accurate than humans at detecting sexual orientation from facial images. J. Pers. Soc. Psychol. 114, 246–257 (2018).
2. J. Koebler, The viral ‘study’ about runners spreading coronavirus is not actually a study. Motherboard (9 April 2020). https://www.vice.com/en/article/v74az9/the-viral-study-about-runners-spreading-coronavirus-is-not-actually-a-study. Accessed 7 December 2020.
3. LA County Public Health, USC-LA county study: Early results of antibody testing suggest number of COVID-19 infections far exceeds number of confirmed cases in Los Angeles County (20 April 2020). publichealth.lacounty.gov/phcommon/public/media/mediapubhpdetail.cfm?prid=2328. Accessed 16 September 2020.
4. F. Gonon, E. Bezard, T. Boraud, Misrepresentation of neuroscience data might give rise to misleading conclusions in the media: The case of attention deficit hyperactivity disorder. PloS One 6, e14618 (2011).
5. A. Yavchitz et al., Misrepresentation of randomized controlled trials in press releases and news coverage: A cohort study. PLoS Med. 9, e1001308 (2012).
6. S. S. Cofield, R. V. Corona, D. B. Allison, Use of causal language in observational studies of obesity and nutrition. Obesity facts 3, 353–356 (2010).
7. N. Haber et al., Causal language and strength of inference in academic and media articles shared in social media (claims): A systematic review. PloS One 13, e0196346 (2018).
8. P. Sumner et al., The association between exaggeration in health related science news and academic press releases: Retrospective observational study. BMJ 349, g7015 (2014).
9. P. Sumner et al., Exaggerations and caveats in press releases and health-related science news. PloS One 11, e0168217 (2016).
10. L. Bratton et al., The association between exaggeration in health-related science news and academic press releases: A replication study. Wellcome Open Res. 4, 148 (2019).
11. R. C. Adams et al., Claims of causality in health news: A randomised trial. BMC Med. 17, 91 (2019).
12. L. M. Schwartz, S. Woloshin, A. Andrews, T. A. Stukel, Influence of medical journal press releases on the quality of associated newspaper coverage: Retrospective cohort study. BMJ 344, d8164 (2012).
13. D. Romer, K. H. Jamieson, S. Aday, Television news and the cultivation of fear of crime. J. Commun. 53, 88–104 (2003).
14. J. Best, G. T. Horiuchi, The razor blade in the apple: The social construction of urban legends. Soc. Probl. 32, 488–499 (1985).
15. H. A. Bajwa, Needle ingestion via Halloween caramel apples. Mayo Clinic Proc. 78, 1311–1312 (2003).
16. P. Kendall, R. Kozial, H. Dardick, Urban yarn of ‘mall slasher’ just won’t die. Chicago Tribute, 11 October 1991, News.
17. A. Schroeder, A decades-old ‘slasher’ tale is circulating on TikTok now. Daily Dot (19 June 2020). https://www.dailydot.com/unclick/slasher-under-car-tiktok-kidnapping/. Accessed 7 December 2020.
18. R. Rahal et al., Estimating the reproducibility of psychological science. Science 349, aac4716 (2015).
19. C. F. Camerer et al., Evaluating replicability of laboratory experiments in economics. Science 351, 1433–1436 (2016).
20. C. G. Begley, L. M. Ellis, Drug development: Raise standards for preclinical cancer research. Nature 483, 531–533 (2012).
21. T. M. Errington et al., An open investigation of the reproducibility of cancer biology research. eLife 3, e04333 (2014).
22. S. Ebrahim et al., Reanalyses of randomized clinical trial data. J. Am. Med. Assoc. 312, 1024–1032 (2014).
23. T. D. Sterling, Publication decisions and their possible effects on inferences drawn from tests of significance—or vice versa. J. Am. Stat. Assoc. 54, 30–34 (1959).
24. R. Rosenthal, The file drawer problem and tolerance for null results. Psychol. Bull. 86, 638–641 (1979).
25. S. B. Nissen, T. Magidson, K. Gross, C. T. Bergstrom, Publication bias and the canonization of false facts. eLife 5, e21451 (2016).
26. D. Fanelli, Negative results are disappearing from most disciplines and countries. Scientometrics 90, 891–904 (2012).
27. E. H. Turner, A. M. Matthews, E. Linardatos, R. A. Tell, R. Rosenthal, Selective publication of antidepressant trials and its influence on apparent efficacy. N. Engl. J. Med. 358, 252–260 (2008).
28. B. A. Nosek, D. Lakens, Registered reports: A method to increase the credibility of published results. Soc. Psychol. 45, 137–141 (2014).
29. T. E. Hardwicke, J. P. Ioannidis, Mapping the universe of registered reports. Nat. Hum. Behav. 2, 793–796 (2018).
30. National Academies of Sciences, Engineering, and Medicine, Reproducibility and replicability in science (2019). https://www.nationalacademies.org/our-work/reproducibility-and-replicability-in-science. Accessed 7 December 2020.
31. R. M. Shiffrin, K. B. Borner, S. M. Stigler, Scientific progress despite irreproducibility: A seeming paradox. Proc. Natl. Acad. Sci. U.S.A. 115, 2632–2639 (2018).
32. S. Goldin-Meadow, Why preregistration makes me nervous. APS Observer 29, 5–6 (2016).
33. A. Marcus, The science of this pandemic is moving at dangerous speeds. Wired, 28 March 2020. https://www.wired.com/story/the-science-of-this-pandemic-is-moving-at-dangerous-speeds/. Accessed 7 December 2020.
34. C. Cookson, Coronavirus may have infected half of UK population—Oxford study. Financial Times, 24 March 2020. https://www.ft.com/content/5ff6469a-6dd8-11ea-89df-41bea055720b. Accessed 7 December 2020.
35. J. Lourenço et al., Fundamental principles of epidemic spread highlight the immediate need for large-scale serological surveys to assess the stage of the SARS-CoV-2 epidemic. https://doi.org/10.1101/2020.03.24.20042291 (26 March 2020).
36. E. Dumas-Mallet, A. Smith, T. Boraud, F. Gonon, Poor replication validity of biomedical association studies reported by newspapers. PloS One 12, e0172650 (2017).
37. D. P. Phillips, E. J. Kanter, B. Bednarczyk, P. L. Tastad, Importance of the lay press in the transmission of medical knowledge to the scientific community. N. Engl. J. Med. 325, 1180–1183 (1991).
38. V. Spezi, Is information-seeking behavior of doctoral students changing?: A review of the literature (2010–2015). New Rev. Acad. Librarian. 22, 78–106 (2016).
39. D. Nicholas et al., Where and how early career researchers find scholarly information. Learn. Publ. 30, 19–29 (2017).
40. P. S. Anderson et al., A case study exploring associations between popular media attention of scientific research and scientific citations. PloS One 15, e0234912 (2020).
41. R. Allen, Survey finds foreign students aren’t applying to American colleges. NBC News, 25 March 2017. https://www.nbcnews.com/nightly-news/survey-finds-foreign-students-aren-t-applying-american-colleges-n738411. Accessed 7 December 2020.
42. American Association of Collegiate Registrars and Admissions Officers, International Student Applications Decline, Concerns about Visas and U.S. Political Climate Rise (AACRAO, 2017).
43. P. A. Todd, J. R. Guest, J. Lu, L. M. Chou, One in four citations in marine biology papers is inappropriate. Mar. Ecol. Prog. Ser. 408, 299–303 (2010).
44. H. Jergas, C. Baethge, Quotation accuracy in medical journal articles—a systematic review and meta-analysis. PeerJ 3, e1364 (2015).
45. G. De Lacey, C. Record, J. Wade, How accurate are quotations and references in medical journals?. Br. Med. J. 291, 884–886 (1985).
46. M. A. M. Iesa, Medical students’ perception of their education and training to cope with future market trends [retraction]. Adv. Med. Educ. Pract. 11, 337–338 (2020).
47. J. Porter, H. Jick, Addiction rare in patients treated with narcotics. N. Engl. J. Med. 302, 123 (1980).
48. P. T. Leung, E. M. Macdonald, M. B. Stanbrook, I. A. Dhalla, D. N. Juurlink, A 1980 letter on the risk of opioid addiction. N. Engl. J. Med. 376, 2194–2195 (2017).
49. D. G. Hamilton, Continued citation of retracted radiation oncology literature—do we have a problem? Int. J. Radiat. Oncol. Biol. Phys. 103, 1036–1042 (2019).
50. S. A. Greenberg, How citation distortions create unfounded authority: Analysis of a citation network. BMJ 339, b2680 (2009).
51. A. S. Jannot, T. Agoritsas, A. Gayet-Ageron, T. V. Perneger, Citation bias favoring statistically significant studies was present in medical research. J. Clin. Epidemiol. 66, 296–301 (2013).
52. J. A. Bastiaansen, Y. A. de Vries, M. R. Munafó, Citation distortions in the literature on the serotonin-transporter-linked polymorphic region and amygdala activation. Biol. Psychiatr. 78, E35–E36 (2015).
53. B. Duyx, M. J. Urlings, G. M. Swaen, L. M. Bouter, M. P. Zeegers, Scientific citations favor positive results: A systematic review and meta-analysis. J. Clin. Epidemiol. 88, 92–101 (2017).
54. S. Subramanian, Inside the Macedonian fake-news complex. Wired, 15 February 2017. https://www.wired.com/2017/02/veles-macedonia-fake-news/. Accessed 7 December 2020.
55. T. C. Bergstrom, C. T. Bergstrom, Can ‘author pays’ journals compete with ‘reader pays?’ Nature (2004).
56. J. D. West, T. Bergstrom, C. T. Bergstrom, Cost effectiveness of open access publications. Econ. Inq. 52, 1315–1321 (2014).
57. C. Shen, B. C. Björk, Predatory open access: A longitudinal study of article volumes and market characteristics. BMC Med. 13, 230 (2015).
58. D. Pyne, The rewards of predatory publications at a small business school. J. Sch. Publish. 48, 137–160 (2017).
59. J. Beall, Medical publishing and the threat of predatory journals. Int. J. Womens. Dermatol. 2, 115–116 (2016).
60. G. J. Martin, A. Yurukoglu, Bias in cable news: Persuasion and polarization. Am. Econ. Rev. 107, 2565–2599 (2017).
61. W. Quattrociocchi, A. Scala, C. R. Sunstein, Echo chambers on Facebook. dx.doi.org/10.2139/ssrn.2795110 (13 June 2016).
62. M. Haim, A. Graefe, H. B. Brosius, Burst of the filter bubble? Effects of personalization on the diversity of Google News. Digit. J. 6, 330–343 (2018).
63. S. Flaxman, S. Goel, J. M. Rao, Filter bubbles, echo chambers, and online news consumption. Publ. Opin. Q. 80, 298–320 (2016).
64. R. K. Garrett, Echo chambers online?: Politically motivated selective exposure among internet news users. J. Comput. Mediated Commun. 14, 265–285 (2009).
65. G. Eady, J. Nagler, A. Guess, J. Zilinsky, J. A. Tucker, How many people live in political bubbles on social media? Evidence from linked survey and twitter data. SAGE Open 9, 2158244019832705 (2019).
66. R. Spier, The history of the peer-review process. Trends Biotech. 20, 357–358 (2002).
67. L. Kim, J. Portenoy, J. D. West, K. Stovel, Scientific journals still matter in the era of academic search engines and preprint archives. JASIST 71, 1218–1226 (2020).
68. V. Larivière, Y. Gingras, E. Archambault, The decline in the concentration of citations, 1900–2007. JASIST 60, 858–862 (2009).
69. R. K. Merton, The Matthew effect in science: The reward and communication systems of science are considered. Science 159, 56–63 (1968).
70. R. K. Merton, The Matthew effect in science. II. Cumulative advantage and the symbolism of intellectual property. Isis 79, 606–623 (1988).
71. L. Kim, C. Adolph, J. D. West, K. Stovel, The influence of changing marginals on measures of inequality in scholarly citations: Evidence of bias and a resampling correction. Soc. Sci. 7, 314–341 (2020).
72. J. A. Evans, Electronic publication and the narrowing of science and scholarship. Science 321, 395–399 (2008).
73. R. K. Pan, A. M. Petersen, F. Pammolli, S. Fortunato, The memory of science: Inflation, myopia, and the knowledge network. J. Inform. 12, 656–678 (2018).





# References

1. K. J. Zollman, The epistemic benefit of transient diversity. Erkenntnis 72, 17 (2010).
2. J. Weatherall, C. O’Connor, Conformity in scientific networks. arXiv:1803.09905v1 (27 March 2018).
3. K. R. Larsen, D. Hovorka, A. Dennis, J. D. West, Understanding the elephant: The discourse approach to boundary identification and corpus construction for theory review articles. J. Assoc. Inf. Syst. Online 20, 15 (2019).
4. M. Gusenbauer, Google Scholar to overshadow them all? Comparing the sizes of 12 academic search engines and bibliographic databases. Scientometrics 118, 177–214 (2019).
5. C. T. Bergstrom, J. D. West, Vizualization: Misleading axes on graphs (2017). https://www.callingbullshit.org/tools/tools misleading axes.html. Accessed 7 December 2020.
6. C. T. Bergstrom, J. D. West, Vizualization: The principle of proportional ink (2017) https://www.callingbullshit.org/tools/tools proportional ink.html. Accessed 7 December 2020.
7. A. Cairo, How Charts Lie. (WW Norton & Company, 2019).
8. J. Roozenbeek et al., Susceptibility to misinformation about COVID-19 around the world. R. Soc. Open. Sci. 7, 201199 (2020).
9. R. A. Muller, Physics for Future Presidents: The Science behind the Headlines (WW Norton & Company, 2008).
10. M. A. Hernán, S. Hernández-Díaz, J. M. Robins, A structural approach to selection bias. Epidemiology 15, 615–625 (2004).
11. S. W. Lagakos, General right censoring and its impact on the analysis of survival data. Biometrics 35, 139–156 (1979).
12. C. H. Wagner, Simpson’s paradox in real life. Am. Statistician 36, 46–48 (1982).
13. A. R. Feinstein, D. M. Sosin, C. K. Wells, The Will Rogers phenomenon: Stage migration and new diagnostic techniques as a source of misleading statistics for survival in cancer. N. Engl. J. Med. 312, 1604–1608 (1985).
14. N. Bostrom, Anthropic Bias: Observation Selection Effects in Science and Philosophy (Routledge, 2013).
15. R. L. Wasserstein, N. A. Lazar, The ASA statement on p-values: Context, process, and purpose. Am. Stat. 70, 129–133 (2016).
16. T. Yarkoni, The generalizability crisis (2019). https://psyarxiv.com/jqw35. Accessed 7 December 2020.
17. R. N. Proctor, L. Schiebinger, Eds. Agnotology: The Making and Unmaking of Ignorance (Stanford University Press, Stanford, CA, 2008).
18. N. Oreskes, E. M. Conway, Merchants of Doubt: How a Handful of Scientists Obscured the Truth on Issues from Tobacco Smoke to Global Warming (Bloomsbury Publishing USA, 2011).
19. C. Paul, M. Matthews, The Russian “Firehose of Falsehood” Propaganda Model: Why It Might Work and Options to Counter It. RAND Corporation (2016). https://www.rand.org/pubs/perspectives/PE198.html. Accessed 7 December 2020.
20. D. Fallis, What is disinformation? Libr. Trends 63, 401–426 (2015).
21. K. Starbird, A. Arif, T. Wilson, “Disinformation as collaborative work: Surfacing the participatory nature of strategic information operations” in Proceedings of the ACM on Human-Computer Interaction (ACM, 2019), vol. 3, pp. 1–26 (2019).
22. A. Arif, L. G. Stewart, K. Starbird, Acting the part: Examining information operations within #blacklivesmatter discourse. Proc. ACM Hum. Comput. Interaction 2, 20 (2018).
23. L. Tran, Firehosing: The systemic strategy that anti-vaxxers are using to spread misinformation. The Guardian, 7 November 2019. https://www.theguardian.com/commentisfree/2019/nov/07/firehosing-the-systemic-strategy-that-anti-vaxxers-are-using-to-spread-misinformation. Accessed 7 December 2020.
24. A. K. Wood, A. M. Ravel, Fool me once: Regulating fake news and other online advertising. S. Cal. L. Rev. 91, 1223 (2017).
25. K. Shu, A. Sliva, S. Wang, J. Tang, H. Liu, Fake news detection on social media: A data mining perspective. ACM SIGKDD Explorations Newsletter 19, 22–36 (2017).
26. S. McGrew, T. Ortega, J. Breakstone, S. Wineburg, The challenge that’s bigger than fake news: Civic reasoning in a social media environment. Am. Educat. 41, 4 (2017).
27. D. Partha, P. A. David, Toward a new economics of science. Res. Pol. 23, 487–521 (1994).
28. P. E. Stephan, The economics of science. J. Econ. Lit. 34, 1199–1235 (1996).
29. P. Mirowski, E. M. Sent, Science Bought and Sold: Essays in the Economics of Science (University of Chicago Press, 2002).
30. J. D. West, How to improve the use of metrics: Learn from game theory. Nature 465, 870–872 (2010).
31. D. Van Dijk, O. Manor, L. B. Carey, Publication metrics and success on the academic job market. Curr. Biol. 24, R516–R517 (2014).
32. F. Brischoux, F. Angelier, Academia’s never-ending selection for productivity. Scientometrics 103, 333–336 (2015).
33. M. Strathern, ‘Improving ratings’: Audit in the British university system. Eur. Rev. 5, 305–321 (1997).
34. D. Geman, S. Geman, Opinion: Science in the age of selfies. Proc. Natl. Acad. Sci. U.S.A. 113, 9384–9387 (2016).
35. M. L. Head, L. Holman, R. Lanfear, A. T. Kahn, M. D. Jennions, The extent and consequences of p-hacking in science. PLoS Biol. 13, e1002106 (2015).
36. N. L. Kerr, Harking: Hypothesizing after the results are known. Pers. Soc. Psychol. Rev. 2, 196–217 (1998).
37. J. P. Simmons, L. D. Nelson, U. Simonsohn, False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychol. Sci. 22, 1359–1366 (2011).
38. S. Haustein, V. Larivière, “The use of bibliometrics for assessing research: Possibilities, limitations and adverse effects” in Incentives and Performance: Governance of Research Organizations, I. M. Welpe, J. Wollersheim, S. Ringelhan, M. Osterloh, Eds. (Springer, Cham, Switzerland, 2015), pp. 121–139.
39. A. D. Higginson, M. R. Munafò, Current incentives for scientists lead to underpowered studies with erroneous conclusions. PLoS Biol. 14, e2000995 (2016).
40. P. E. Smaldino, R. McElreath, The natural selection of bad science. R. Soc. Open Sci. 3, 160384 (2016).
41. A. J. Stewart, J. B. Plotkin, The natural selection of good science. arXiv:2003.00928 (2 March 2020).
42. D. Siegel, P. Baveye, Battling the paper glut. Science 329, 1466 (2010).
43. M. C. Frank, N-best evaluation for academic hiring and promotion. Trends Cognit. Sci. 23, 983–985 (2019).
44. N. J. Brown, J. A. Heathers, The grim test: A simple technique detects numerous anomalies in the reporting of results in psychology. Soc. Psychol. Personal. Sci. 8, 363–369 (2017).
45. I. Wesley-Smith, C. T. Bergstrom, J. D. West, Static ranking of scholarly papers using article-level eigenfactor (alef). arXiv:1606.08534 (28 June 2016).
46. M. Waldrop, “AAAS: Science journalism in crisis?” Nature. In the field (14 February 2009). http://blogs.nature.com/inthefield/2009/02/aaas science journalism in cri.html. Accessed 7 December 2020.
47. C. T. Bergstrom, J. D. West, Calling Bullshit: The Art of Skepticism in a Data-Driven World (Random House, 2020).
48. K. Parker, The growing partisan divide in views of higher education. Pew Research Center (19 August 2019). https://www.pewsocialtrends.org/essay/the-growing-partisan-divide-in-views-of-higher-education/. Accessed 7 December 2020.
49. J. Gramlich, Young Americans are less trusting of other people – and key institutions – than their elders. Pew Research Center (6 August 2019). https://www.pewresearch.org/fact-tank/2019/08/06/young-americans-are-less-trusting-of-other-people-and-key-institutions-than-their-elders/. Accessed 7 December 2020.
50. C. Funk, Mixed messages about public trust in science. Issues Sci. Technol. 34, 86–88 (2017).
51. C. Funk, B. Kennedy, Public confidence in scientists has remained stable for decades. Pew Research Center (27 August 2020). https://www.pewresearch.org/fact-tank/2020/08/27/public-confidence-in-scientists-has-remained-stable-for-decades/. Accessed 7 December 2020.
52. N. M. Krause, D. Brossard, D. A. Scheufele, M. A. Xenos, K. Franke, Trends—Americans’ Trust in Science and Scientists. Publ. Opin. Q. 83, 817–836 (2019).
53. N. S. Board, Science and Engineering Indicators (NSF, 2018), vol. 1.
54. G. Gauchat, Politicization of science in the public sphere: A study of public trust in the United States, 1974 to 2010. Am. Socio. Rev. 77, 167–187 (2012).
55. S. Iyengar, D. S. Massey, Scientific communication in a post-truth society. Proc. Natl. Acad. Sci. U.S.A. 116, 7656–7661 (2019).